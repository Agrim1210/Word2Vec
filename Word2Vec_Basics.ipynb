{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Word2Vec Model\n",
    "- Word2Vec Google's Pretrained Model\n",
    "- Contains vector representations of 50 billion words\n",
    "\n",
    "- Words which are similar in context have similar vectors\n",
    "- Distance/Similarity between two words can be measured using Cosine Distance\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Applications\n",
    "- Text Similarity\n",
    "- Language Translation\n",
    "- Finding Odd Words\n",
    "- Word Analogies\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word Embeddings\n",
    "- Word embeddings are numerical representation of words, in the form of vectors.\n",
    "\n",
    "- Word2Vec Model represents each word as 300 Dimensional Vector\n",
    "\n",
    "- In this tutorial we are going to see how to use pre-trained word2vec model.\n",
    "- Model size is around 1.5 GB\n",
    "- We will work using Gensim, which is popular NLP Package.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Gensim's Word2Vec Model provides optimum implementation of \n",
    "\n",
    "1) **CBOW** Model \n",
    "\n",
    "2) **SkipGram Model**\n",
    "\n",
    "\n",
    "Paper 1 [Efficient Estimation of Word Representations in\n",
    "Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "\n",
    "\n",
    "Paper 2 [Distributed Representations of Words and Phrases and their Compositionality\n",
    "](https://arxiv.org/abs/1310.4546)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Word2Vec using Gensim\n",
    "`Link https://radimrehurek.com/gensim/models/word2vec.html`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### CODE ##"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load Word2Vec Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**KeyedVectors** - This object essentially contains the mapping between words and embeddings. After training, it can be used directly to query those embeddings in various ways"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import gensim\r\n",
    "from gensim.models import word2vec\r\n",
    "from gensim.models import KeyedVectors\r\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format(\r\n",
    "    'GoogleNews-vectors-negative300.bin', binary=True)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "vector_apple = word_vectors['apple']\r\n",
    "vector_mango = word_vectors['mango']\r\n",
    "print(vector_apple.shape, vector_mango.shape)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(300,) (300,)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "cosine_similarity([vector_apple],[vector_mango])"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.57518554]], dtype=float32)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Find the Odd One Out\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "input_1 = [\"apple\",\"mango\",\"juice\",\"party\",\"orange\"] \r\n",
    "input_2 = [\"music\",\"dance\",\"sleep\",\"dancer\",\"food\"]        \r\n",
    "input_3  = [\"match\",\"player\",\"football\",\"cricket\",\"dancer\"]\r\n",
    "input_4 = [\"india\",\"paris\",\"russia\",\"france\",\"germany\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "\r\n",
    "def odd_one_out(words):\r\n",
    "    \"\"\"Accepts a list of words and returns the odd word\r\n",
    "       Lesser the cosine similarity , more chance of being odd one out\r\n",
    "    \"\"\"\r\n",
    "    # Generate all word embedding for the given list\r\n",
    "    all_word_vectors = [word_vectors[w] for w in words]\r\n",
    "    avg_vector = np.mean(all_word_vectors, axis=0)\r\n",
    "    print(avg_vector.shape)\r\n",
    "\r\n",
    "    # Iterate over every word and find similarity\r\n",
    "    odd_one_out = None\r\n",
    "    min_similarity = 1.0\r\n",
    "    for w in words:\r\n",
    "       sim = cosine_similarity([word_vectors[w]], [avg_vector])\r\n",
    "       if sim < min_similarity:\r\n",
    "          min_similarity = sim\r\n",
    "          odd_one_out = w\r\n",
    "       print(\"Similairy btw %s and avg vector is %.2f\"%(w,sim))\r\n",
    "    return odd_one_out\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "odd_one_out(input_4)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(300,)\n",
      "Similairy btw india and avg vector is 0.81\n",
      "Similairy btw paris and avg vector is 0.75\n",
      "Similairy btw russia and avg vector is 0.79\n",
      "Similairy btw france and avg vector is 0.81\n",
      "Similairy btw germany and avg vector is 0.84\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'paris'"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Word Analogies Task"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the word analogy task, we complete the sentence \"a is to b as c is to __\". An example is 'man is to woman as king is to queen' . In detail, we are trying to find a word d, such that the associated word vectors `ea,eb,ec,ed` are related in the following manner: `eb−ea≈ed−ec`. We will measure the similarity between `eb−ea` and `ed−ec` using cosine similarity. \r\n",
    "\r\n",
    "![Word2Vec](word2vec.png)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`man -> woman :: \tprince -> princess`  \n",
    "`italy -> italian :: \tspain -> spanish`  \n",
    "`india -> delhi :: \tjapan -> tokyo`  \n",
    "`man -> woman :: \tboy -> girl`  \n",
    "`small -> smaller :: \tlarge -> larger`  \n",
    "\n",
    "#### Try it out \n",
    "\n",
    "\n",
    "`man -> coder :: woman -> ______?`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "type(word_vectors.key_to_index)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def predict_word(a, b, c, word_vectors):\r\n",
    "    \"\"\"Accepts a triad of words, a,b,c and returns d such that a is to b : c is to d\"\"\"\r\n",
    "    a, b, c = a.lower(), b.lower(), c.lower()\r\n",
    "\r\n",
    "    # Similarity between |b-a|=|d-c| should be maximum\r\n",
    "    max_similarity = -100\r\n",
    "    d = None\r\n",
    "    words = word_vectors.key_to_index.keys()\r\n",
    "\r\n",
    "    wa, wb, wc = word_vectors[a], word_vectors[b], word_vectors[c]\r\n",
    "    # to find d s.t similarity(|b-a|,|d-c|) should be max\r\n",
    "\r\n",
    "    for w in words:\r\n",
    "        if w in [a, b, c]:\r\n",
    "            continue\r\n",
    "\r\n",
    "        wv = word_vectors[w]\r\n",
    "        sim = cosine_similarity([wb-wa], [wv-wc])\r\n",
    "\r\n",
    "        if sim > max_similarity:\r\n",
    "            max_similarity = sim\r\n",
    "            d = w\r\n",
    "    return d\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "triad_2 = (\"man\",\"woman\",\"prince\")\r\n",
    "predict_word(*triad_2,word_vectors)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'princess'"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Using the Most Similar Method"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "word_vectors.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('queen', 0.7118193507194519)]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Training Your Own Word2Vec Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Word2Vec model can learn embeddings from any text corpus!\n",
    "- Continuous Bag of Words Model\n",
    "- Skip Gram Model\n",
    "\n",
    "`Algorithm looks at window of target word(Y) to provide context word(X), the model is trained on (X,Y) pairs in a superwised manner.` The algorithm was developed by Tomas Mikolov."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preparation\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "- Each sentence must be tokenized, into a list of words.\n",
    "\n",
    "- The sentences can be text loaded into memory once,\n",
    "or we can build a data pipeline which iteratively feeds data to the model.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "import nltk\r\n",
    "from nltk.corpus import stopwords"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "stop_word=set(stopwords.words('english'))\r\n",
    "def readFile(file):\r\n",
    "    f= open(file,'r',encoding='utf8')\r\n",
    "    text=f.read()\r\n",
    "    # Tokenization - sentences and words\r\n",
    "    sentences=nltk.sent_tokenize(text)\r\n",
    "    print(len(sentences))\r\n",
    "    data=[]\r\n",
    "    for sent in sentences:\r\n",
    "        words=nltk.word_tokenize(sent)\r\n",
    "        words=[w.lower() for w in words if len(w)>2 and w not in stop_word ]\r\n",
    "        data.append(words)\r\n",
    "\r\n",
    "    return data\r\n",
    "text=readFile('bollywood.txt')\r\n",
    "print(text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "18\n",
      "[['deepika', 'padukone', 'ranveer', 'singh', 'wedding', 'one', 'biggest', 'bollywood', 'events', 'happened', '2018'], ['the', 'deepika', 'ranveer', 'celebrations', 'hooked', 'phones', 'waiting', 'come', 'also', 'gave', 'enough', 'reason', 'believe', 'stylish', 'two', 'couple'], ['from', 'airport', 'looks', 'reception', 'parties', 'everything', 'entire', 'timeline', 'deepika', 'ranveer', 'wedding', 'style', 'file'], ['not', 'ambanis', 'deepika', 'ranveer', 'priyanka', 'nick'], ['man', 'proves', 'wedding', 'the', 'year', 'this', 'year', 'year', 'big', 'fat', 'lavish', 'extravagant', 'weddings'], ['from', 'isha', 'ambani', 'anand', 'piramal', 'deepika', 'padukone', 'ranveer', 'singh', 'priyanka', 'chopra', 'nick', 'jonas', 'kapil', 'sharma', 'ginni', 'chatrath', '2018', 'saw', 'many', 'grand', 'weddings'], ['but', 'nothing', 'beats', 'man', 'wedding', 'the', 'year', 'award', 'social', 'media'], ['priyanka', 'also', 'shared', 'video', 'featuring', 'nick', 'jonaswas', 'also', 'celebrating', 'the', 'family', 'first', 'celebrated', 'christmas', 'london', 'pictures', 'priyanka', 'chopra', 'nick', 'jonas', 'new', 'year', 'celebrations', 'outstanding'], ['priyanka', 'chopra', 'nick', 'shared', 'glimpses', 'celebration', 'verbier', 'switzerland'], ['priyanka', 'chopra', 'married', 'nick', 'jonas', 'december', 'three', 'wedding', 'receptions', 'one', 'new', 'delhi', 'two', 'mumbai'], ['this', 'year', 'year', 'big', 'fat', 'lavish', 'extravagant', 'weddings'], ['from', 'isha', 'ambani', 'anand', 'piramal', 'deepika', 'padukone', 'ranveer', 'singh', 'priyanka', 'chopra', 'nick', 'jonas', 'kapil', 'sharma', 'ginni', 'chatrath', '2018', 'saw', 'many', 'grand', 'weddings'], ['but', 'nothing', 'beats', 'man', 'wedding', 'the', 'year', 'award', 'social', 'media'], ['kapil', 'sharma', 'ginni', 'chatrath', 'jaggo', 'night', 'december', 'made', 'even', 'special', 'industry', 'friends'], ['kapil', 'sharma', 'ginni', 'chatrath', 'friends', 'long', 'time'], ['there', 'virat', 'side', 'actress', 'wife', 'anushka', 'sharma', 'pleasure', 'audience'], ['while', 'couple', 'rang', 'new', 'year', 'style', 'morning', 'saw', 'virat', 'dress', 'squad', 'attire', 'anushka', 'pink', 'salwar', 'suit'], ['isha', 'ambani', 'married', 'anand', 'piramal', 'year']]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "from gensim.models import Word2Vec\r\n",
    "model=Word2Vec(text,vector_size=300,window=10,min_count=1)\r\n",
    "print(model)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Word2Vec(vocab=116, vector_size=300, alpha=0.025)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "words = list(model.wv.key_to_index.keys())\r\n",
    "print(words)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['year', 'priyanka', 'nick', 'deepika', 'ranveer', 'wedding', 'the', 'chopra', 'sharma', 'ginni', 'weddings', 'jonas', 'kapil', 'chatrath', 'anand', '2018', 'isha', 'ambani', 'piramal', 'saw', 'from', 'new', 'also', 'man', 'singh', 'padukone', 'virat', 'many', 'grand', 'but', 'nothing', 'beats', 'award', 'media', 'shared', 'anushka', 'style', 'couple', 'two', 'social', 'big', 'fat', 'celebrations', 'this', 'december', 'married', 'friends', 'lavish', 'extravagant', 'one', 'entire', 'parties', 'everything', 'timeline', 'file', 'not', 'ambanis', 'pink', 'events', 'happened', 'reception', 'bollywood', 'squad', 'hooked', 'phones', 'waiting', 'come', 'biggest', 'gave', 'enough', 'reason', 'believe', 'stylish', 'attire', 'dress', 'looks', 'airport', 'side', 'morning', 'jaggo', 'celebration', 'verbier', 'switzerland', 'three', 'receptions', 'delhi', 'mumbai', 'night', 'proves', 'made', 'even', 'special', 'industry', 'long', 'time', 'there', 'glimpses', 'outstanding', 'pictures', 'london', 'rang', 'while', 'audience', 'pleasure', 'wife', 'actress', 'salwar', 'video', 'featuring', 'jonaswas', 'celebrating', 'family', 'first', 'celebrated', 'christmas', 'suit']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "actors = [\"ranveer\",\"deepika\",\"padukone\",\"singh\",\"nick\",\"jonas\",\"chopra\",\"priyanka\",\"virat\",\"anushka\",\"ginni\"]\r\n",
    "\r\n",
    "\r\n",
    "def predict_actor(a,b,c,word_vectors):\r\n",
    "    \"\"\"Accepts a triad of words, a,b,c and returns d such that a is to b : c is to d\"\"\"\r\n",
    "    a,b,c = a.lower(),b.lower(),c.lower()\r\n",
    "    max_similarity = -100 \r\n",
    "    \r\n",
    "    d = None\r\n",
    "    words = actors\r\n",
    "    \r\n",
    "    wa,wb,wc = word_vectors[a],word_vectors[b],word_vectors[c]\r\n",
    "    \r\n",
    "    #to find d s.t similarity(|b-a|,|d-c|) should be max\r\n",
    "    \r\n",
    "    for w in words:\r\n",
    "        if w in [a,b,c]:\r\n",
    "            continue\r\n",
    "        \r\n",
    "        wv = word_vectors[w]\r\n",
    "        sim = cosine_similarity([wb-wa],[wv-wc])\r\n",
    "        \r\n",
    "        if sim > max_similarity:\r\n",
    "            max_similarity = sim\r\n",
    "            d = w\r\n",
    "    return d"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Test your Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "triad = (\"nick\",\"priyanka\",\"virat\")\r\n",
    "predict_actor(*triad,model.wv)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'padukone'"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "\r\n",
    "triad = (\"ranveer\",\"deepika\",\"priyanka\")\r\n",
    "predict_actor(*triad,model.wv)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'jonas'"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "triad = (\"priyanka\",\"jonas\",\"nick\")\r\n",
    "predict_actor(*triad,model.wv)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'deepika'"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "\r\n",
    "triad = (\"nick\",\"priyanka\",\"virat\")\r\n",
    "predict_actor(*triad,model.wv)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'padukone'"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "triad = (\"ranveer\",\"deepika\",\"priyanka\")\r\n",
    "predict_actor(*triad,model.wv)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'jonas'"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "interpreter": {
   "hash": "247ab06e135bb35fa78c5eff31b2a9a0050dcb5fb773c2631d2a29ac689eeccb"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}